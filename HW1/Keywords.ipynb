{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-rake\n",
    "pip install summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем следующие алгоритмы: RAKE, TextRank, tf-idf. В качестве текстов мы взяли 4 статьи с киберленинки, в первой строчке написаны ключевые слова (из раздела ключевые слова статьи + те что руками нашли)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число токенов: 930\n",
      "Число токенов: 1189\n",
      "Число токенов: 1325\n",
      "Число токенов: 851\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "texts = []\n",
    "keywords = []\n",
    "\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "import RAKE\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from summa import keywords as textrank\n",
    "import numpy as np\n",
    "\n",
    "# Лемматизация текстов\n",
    "m = MorphAnalyzer()\n",
    "def normalize_text(text):\n",
    "    lemmas = []\n",
    "    for t in simple_word_tokenize(text):\n",
    "        if len(t) > 2:\n",
    "            lemmas.append(\n",
    "                m.parse(t)[0].normal_form\n",
    "            )\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "\n",
    "for fn in os.listdir('texts'):\n",
    "    if fn.endswith('.txt'):\n",
    "        with open('texts/' + fn, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            keywords.append(list(map(lambda x: normalize_text(x.strip()), lines[0].split(','))))\n",
    "            texts.append(normalize_text(\"\\n\".join(lines[1:])))\n",
    "            print(f\"Число токенов: {len(texts[-1].split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "тут пара вспомогательных функций для проверки принадлежности ключевой фразы списку морфологических паттернов, а так же функция извлечения ключевых слов при помощи tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_pos(pos, tokens):\n",
    "    words = tokens.split()\n",
    "    if len(pos) != len(words):\n",
    "        return False\n",
    "    for i in range(len(pos)):\n",
    "        if pos[i] != m.parse(words[i])[0].tag.POS:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "morph_patterns = [['NOUN'], ['ADJF'], ['ADJS'], ['VERB'], ['ADJF', 'NOUN'], ['VERB', 'NOUN'],\n",
    "                  ['NOUN', 'NOUN'], ['NOUN', 'VERB']]\n",
    "def match_patterns(tokens):\n",
    "    for patt in morph_patterns:\n",
    "        if match_pos(patt, tokens):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_keywords(texts, treshold=0.1, stop_words=[]):\n",
    "    tfidfVectorizer = TfidfVectorizer(stop_words=stop, min_df=2, ngram_range=(1,3))\n",
    "    tfidf_keys = tfidfVectorizer.fit_transform(np.array(list(map(normalize_text, texts)))).todense()\n",
    "    inv_voc = {value: key for key, value in tfidfVectorizer.vocabulary_.items()}\n",
    "    extracted_keywords = []\n",
    "    for i, text in enumerate(texts):\n",
    "        extracted_keywords.append([])\n",
    "        for j in range(tfidf_keys.shape[1]):\n",
    "            if tfidf_keys[i, j] > treshold and inv_voc[j] not in stop_words:\n",
    "                extracted_keywords[-1].append(inv_voc[j])\n",
    "    return extracted_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем немного непревычно считать метрики -- сравнивая извлеченную ключевую фразу с эталонной, мы будем смотреть не точное совпадение, а вхождние одной как подстроки в другую, это отчасти позволит сравнивать их как \"одно и то же по смыслу\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Без морфологических шаблонов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТЕКСТ 0\n",
      "Точность: 0.375\n",
      "Полнота: 0.23076923076923078\n",
      "F-score: 0.2857142857142857\n",
      "-------------------------------------------\n",
      "ТЕКСТ 1\n",
      "Точность: 0.10810810810810811\n",
      "Полнота: 0.3333333333333333\n",
      "F-score: 0.163265306122449\n",
      "-------------------------------------------\n",
      "ТЕКСТ 2\n",
      "Точность: 0.03333333333333333\n",
      "Полнота: 0.125\n",
      "F-score: 0.052631578947368425\n",
      "-------------------------------------------\n",
      "ТЕКСТ 3\n",
      "Точность: 0.0\n",
      "Полнота: 0.0\n",
      "F-score: 0\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('russian')\n",
    "rake = RAKE.Rake(stop)\n",
    "extracted_keywords = []\n",
    "\n",
    "for text in texts:\n",
    "    extracted_keywords_ = rake.run(text, maxWords=3, minFrequency=1)\n",
    "    extracted_keywords.append(list(map(lambda x: x[0], filter(lambda x: x[1] > 1, extracted_keywords_))))\n",
    "\n",
    "    \n",
    "for i, etalon in enumerate(keywords):\n",
    "    tp_count = 0\n",
    "    used_words = set()\n",
    "    for word in extracted_keywords[i]:\n",
    "        for second_word in etalon:\n",
    "            if second_word not in used_words and (word in second_word or second_word in word):\n",
    "                used_words.add(second_word)\n",
    "                tp_count += 1\n",
    "                break\n",
    "                \n",
    "    pr = tp_count/len(extracted_keywords[i])\n",
    "    rec = tp_count/len(etalon)\n",
    "    try:\n",
    "        f_score = 2*pr*rec/(pr + rec)\n",
    "    except ZeroDivisionError:\n",
    "        f_score = 0\n",
    "    print(f\"ТЕКСТ {i}\")\n",
    "    print(f\"Точность: {pr}\")\n",
    "    print(f\"Полнота: {rec}\")\n",
    "    print(f\"F-score: {f_score}\")\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### С морфологическими шаблонами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТЕКСТ 0\n",
      "Точность: 0.0\n",
      "Полнота: 0.0\n",
      "F-score: 0\n",
      "-------------------------------------------\n",
      "ТЕКСТ 1\n",
      "Точность: 0.18181818181818182\n",
      "Полнота: 0.16666666666666666\n",
      "F-score: 0.17391304347826086\n",
      "-------------------------------------------\n",
      "ТЕКСТ 2\n",
      "Точность: 0.0\n",
      "Полнота: 0.0\n",
      "F-score: 0\n",
      "-------------------------------------------\n",
      "ТЕКСТ 3\n",
      "Точность: 0.0\n",
      "Полнота: 0.0\n",
      "F-score: 0\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('russian')\n",
    "rake = RAKE.Rake(stop)\n",
    "extracted_keywords = []\n",
    "\n",
    "for text in texts:\n",
    "    extracted_keywords_ = rake.run(text, maxWords=3, minFrequency=1)\n",
    "    extracted_keywords.append(list(filter(match_patterns, map(lambda x: x[0], filter(lambda x: x[1] > 1, extracted_keywords_)))))\n",
    "\n",
    "    \n",
    "for i, etalon in enumerate(keywords):\n",
    "    tp_count = 0\n",
    "    used_words = set()\n",
    "    for word in extracted_keywords[i]:\n",
    "        for second_word in etalon:\n",
    "            if second_word not in used_words and (word in second_word or second_word in word):\n",
    "                used_words.add(second_word)\n",
    "                tp_count += 1\n",
    "                break\n",
    "                \n",
    "    pr = tp_count/len(extracted_keywords[i])\n",
    "    rec = tp_count/len(etalon)\n",
    "    try:\n",
    "        f_score = 2*pr*rec/(pr + rec)\n",
    "    except ZeroDivisionError:\n",
    "        f_score = 0\n",
    "    print(f\"ТЕКСТ {i}\")\n",
    "    print(f\"Точность: {pr}\")\n",
    "    print(f\"Полнота: {rec}\")\n",
    "    print(f\"F-score: {f_score}\")\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Без морфологических шаблонов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТЕКСТ 0\n",
      "Точность: 0.11392405063291139\n",
      "Полнота: 0.6923076923076923\n",
      "F-score: 0.1956521739130435\n",
      "-------------------------------------------\n",
      "ТЕКСТ 1\n",
      "Точность: 0.08823529411764706\n",
      "Полнота: 0.5\n",
      "F-score: 0.15\n",
      "-------------------------------------------\n",
      "ТЕКСТ 2\n",
      "Точность: 0.030927835051546393\n",
      "Полнота: 0.375\n",
      "F-score: 0.05714285714285715\n",
      "-------------------------------------------\n",
      "ТЕКСТ 3\n",
      "Точность: 0.05063291139240506\n",
      "Полнота: 0.5\n",
      "F-score: 0.09195402298850573\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('russian')\n",
    "extracted_keywords = []\n",
    "\n",
    "for text in texts:\n",
    "    extracted_keywords_ = textrank.keywords(normalize_text(text), language='russian', additional_stopwords=stop, scores=True)\n",
    "    extracted_keywords.append(list(map(lambda x: x[0], extracted_keywords_)))\n",
    "\n",
    "    \n",
    "for i, etalon in enumerate(keywords):\n",
    "    tp_count = 0\n",
    "    used_words = set()\n",
    "    for word in extracted_keywords[i]:\n",
    "        for second_word in etalon:\n",
    "            if second_word not in used_words and (word in second_word or second_word in word):\n",
    "                used_words.add(second_word)\n",
    "                tp_count += 1\n",
    "                break\n",
    "                \n",
    "    pr = tp_count/len(extracted_keywords[i])\n",
    "    rec = tp_count/len(etalon)\n",
    "    try:\n",
    "        f_score = 2*pr*rec/(pr + rec)\n",
    "    except ZeroDivisionError:\n",
    "        f_score = 0\n",
    "    print(f\"ТЕКСТ {i}\")\n",
    "    print(f\"Точность: {pr}\")\n",
    "    print(f\"Полнота: {rec}\")\n",
    "    print(f\"F-score: {f_score}\")\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### С морфологическими шаблонами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТЕКСТ 0\n",
      "Точность: 0.1590909090909091\n",
      "Полнота: 0.5384615384615384\n",
      "F-score: 0.2456140350877193\n",
      "-------------------------------------------\n",
      "ТЕКСТ 1\n",
      "Точность: 0.11764705882352941\n",
      "Полнота: 0.3333333333333333\n",
      "F-score: 0.1739130434782609\n",
      "-------------------------------------------\n",
      "ТЕКСТ 2\n",
      "Точность: 0.04918032786885246\n",
      "Полнота: 0.375\n",
      "F-score: 0.08695652173913043\n",
      "-------------------------------------------\n",
      "ТЕКСТ 3\n",
      "Точность: 0.07142857142857142\n",
      "Полнота: 0.5\n",
      "F-score: 0.125\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('russian')\n",
    "extracted_keywords = []\n",
    "\n",
    "for text in texts:\n",
    "    extracted_keywords_ = textrank.keywords(normalize_text(text), language='russian', additional_stopwords=stop, scores=True)\n",
    "    extracted_keywords.append(list(filter(match_patterns, map(lambda x: x[0], extracted_keywords_))))\n",
    "\n",
    "    \n",
    "for i, etalon in enumerate(keywords):\n",
    "    tp_count = 0\n",
    "    used_words = set()\n",
    "    for word in extracted_keywords[i]:\n",
    "        for second_word in etalon:\n",
    "            if second_word not in used_words and (word in second_word or second_word in word):\n",
    "                used_words.add(second_word)\n",
    "                tp_count += 1\n",
    "                break\n",
    "                \n",
    "    pr = tp_count/len(extracted_keywords[i])\n",
    "    rec = tp_count/len(etalon)\n",
    "    try:\n",
    "        f_score = 2*pr*rec/(pr + rec)\n",
    "    except ZeroDivisionError:\n",
    "        f_score = 0\n",
    "    print(f\"ТЕКСТ {i}\")\n",
    "    print(f\"Точность: {pr}\")\n",
    "    print(f\"Полнота: {rec}\")\n",
    "    print(f\"F-score: {f_score}\")\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Без морфологических шаблонов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТЕКСТ 0\n",
      "Точность: 0.7142857142857143\n",
      "Полнота: 0.38461538461538464\n",
      "F-score: 0.5\n",
      "-------------------------------------------\n",
      "ТЕКСТ 1\n",
      "Точность: 0.17647058823529413\n",
      "Полнота: 0.25\n",
      "F-score: 0.20689655172413793\n",
      "-------------------------------------------\n",
      "ТЕКСТ 2\n",
      "Точность: 0.05263157894736842\n",
      "Полнота: 0.125\n",
      "F-score: 0.07407407407407407\n",
      "-------------------------------------------\n",
      "ТЕКСТ 3\n",
      "Точность: 0.14285714285714285\n",
      "Полнота: 0.375\n",
      "F-score: 0.20689655172413796\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('russian')\n",
    "extracted_keywords = []\n",
    "\n",
    "extracted_keywords = get_keywords(texts, stop_words=stop)\n",
    "\n",
    "for i, etalon in enumerate(keywords):\n",
    "    tp_count = 0\n",
    "    used_words = set()\n",
    "    for word in extracted_keywords[i]:\n",
    "        for second_word in etalon:\n",
    "            if second_word not in used_words and (word in second_word or second_word in word):\n",
    "                used_words.add(second_word)\n",
    "                tp_count += 1\n",
    "                break\n",
    "                \n",
    "    pr = tp_count/len(extracted_keywords[i])\n",
    "    rec = tp_count/len(etalon)\n",
    "    try:\n",
    "        f_score = 2*pr*rec/(pr + rec)\n",
    "    except ZeroDivisionError:\n",
    "        f_score = 0\n",
    "    print(f\"ТЕКСТ {i}\")\n",
    "    print(f\"Точность: {pr}\")\n",
    "    print(f\"Полнота: {rec}\")\n",
    "    print(f\"F-score: {f_score}\")\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### С морфологическими шаблонами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТЕКСТ 0\n",
      "Точность: 0.7142857142857143\n",
      "Полнота: 0.38461538461538464\n",
      "F-score: 0.5\n",
      "-------------------------------------------\n",
      "ТЕКСТ 1\n",
      "Точность: 0.25\n",
      "Полнота: 0.25\n",
      "F-score: 0.25\n",
      "-------------------------------------------\n",
      "ТЕКСТ 2\n",
      "Точность: 0.0625\n",
      "Полнота: 0.125\n",
      "F-score: 0.08333333333333333\n",
      "-------------------------------------------\n",
      "ТЕКСТ 3\n",
      "Точность: 0.17647058823529413\n",
      "Полнота: 0.375\n",
      "F-score: 0.24\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('russian')\n",
    "extracted_keywords = []\n",
    "\n",
    "extracted_keywords = get_keywords(texts, stop_words=stop)\n",
    "for i in range(len(extracted_keywords)):\n",
    "    extracted_keywords[i] = list(filter(match_patterns, extracted_keywords[i]))\n",
    "\n",
    "for i, etalon in enumerate(keywords):\n",
    "    tp_count = 0\n",
    "    used_words = set()\n",
    "    for word in extracted_keywords[i]:\n",
    "        for second_word in etalon:\n",
    "            if second_word not in used_words and (word in second_word or second_word in word):\n",
    "                used_words.add(second_word)\n",
    "                tp_count += 1\n",
    "                break\n",
    "                \n",
    "    pr = tp_count/len(extracted_keywords[i])\n",
    "    rec = tp_count/len(etalon)\n",
    "    try:\n",
    "        f_score = 2*pr*rec/(pr + rec)\n",
    "    except ZeroDivisionError:\n",
    "        f_score = 0\n",
    "    print(f\"ТЕКСТ {i}\")\n",
    "    print(f\"Точность: {pr}\")\n",
    "    print(f\"Полнота: {rec}\")\n",
    "    print(f\"F-score: {f_score}\")\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['большой',\n",
       " 'время',\n",
       " 'высокий',\n",
       " 'деятельность',\n",
       " 'жизнь',\n",
       " 'который',\n",
       " 'работа',\n",
       " 'различный',\n",
       " 'свой',\n",
       " 'ситуация',\n",
       " 'событие',\n",
       " 'социальный',\n",
       " 'степень',\n",
       " 'фактор',\n",
       " 'человек',\n",
       " 'эмоциональный']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_keywords[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Морфологические шаблоны изредка повышают точность, но сильно просаживают полноту и как правило мешаются. Разные алгоритмы хорошо работают на разных текстах, но у всех проблемы со вторым и третьим текстом.\n",
    "\n",
    "Основная проблема как мне кажется - субъективность разметки и степень \"спецефичности\" каждого слова, например в случае с tf-idf мы выделили много слов, которые, формально говоря специфичны для данного текста, но не были указаны в качестве ключевых так как являются слишком общими (работа, свой, ситуация...). Тут может помочь более жесткий набор стоп-слов, либо просто большее число текстов, чтоб idf-состовляющая стала лучше работать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
